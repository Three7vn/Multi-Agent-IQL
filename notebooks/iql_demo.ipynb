{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Multi-Agent Independent Q-Learning (IQL) Demo\n",
        "\n",
        "This notebook demonstrates how to use the Independent Q-Learning (IQL) implementation with multiple agents in a simple gridworld environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "import time\n",
        "\n",
        "# Add the src directory to the path\n",
        "sys.path.append('../')\n",
        "from src import IQLAgent, MultiAgentGridworldEnv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Environment Setup\n",
        "\n",
        "First, let's configure and initialize our multi-agent gridworld environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Environment configuration\n",
        "env_config = {\n",
        "    'grid_size': (5, 5),  # 5x5 grid\n",
        "    'num_agents': 2,      # 2 agents\n",
        "    'agent_start_pos': {0: (0, 0), 1: (4, 4)},  # Agent 0 starts at top-left, Agent 1 at bottom-right\n",
        "    'agent_goal_pos': {0: (4, 4), 1: (0, 0)},   # Agent 0's goal is bottom-right, Agent 1's is top-left\n",
        "    'obstacles_pos': [(2, 2)],  # One obstacle in the middle\n",
        "    'max_steps': 50,            # Maximum steps per episode\n",
        "    'observation_type': 'coords',  # Agents observe their coordinates\n",
        "    'reward_type': 'individual',   # Individual rewards\n",
        "    'slip_prob': 0.0            # No slipping (deterministic)\n",
        "}\n",
        "\n",
        "# Create the environment\n",
        "env = MultiAgentGridworldEnv(**env_config)\n",
        "\n",
        "# Print environment information\n",
        "print(f\"Grid size: {env.grid_height}x{env.grid_width}\")\n",
        "print(f\"Number of agents: {env.num_agents}\")\n",
        "print(f\"Agent start positions: {env.agent_start_pos}\")\n",
        "print(f\"Agent goal positions: {env.agent_goal_pos}\")\n",
        "print(f\"Obstacles: {env.obstacles_pos}\")\n",
        "print(f\"Action space size: {env.action_space_size}\")\n",
        "\n",
        "# Render the initial environment\n",
        "print(\"\\nInitial environment state:\")\n",
        "env.reset()\n",
        "env.render()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Agent Setup\n",
        "\n",
        "Now, let's create our Independent Q-Learning agents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Determine observation dimension based on environment configuration\n",
        "if env.observation_type == 'coords':\n",
        "    obs_dim = 2\n",
        "elif env.observation_type == 'local_grid_3x3':\n",
        "    obs_dim = 9\n",
        "elif env.observation_type == 'full_state':\n",
        "    obs_dim = env.grid_width * env.grid_height\n",
        "else:\n",
        "    raise ValueError(\"Unsupported observation type\")\n",
        "\n",
        "action_dim = env.action_space_size\n",
        "\n",
        "# Hyperparameters\n",
        "BUFFER_CAPACITY = 10000\n",
        "LEARNING_RATE = 1e-4\n",
        "GAMMA = 0.99\n",
        "EPSILON_START = 1.0\n",
        "EPSILON_END = 0.05\n",
        "EPSILON_DECAY = 0.99\n",
        "TARGET_UPDATE_FREQ = 50\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Create agents\n",
        "agents = {i: IQLAgent(i, obs_dim, action_dim, BUFFER_CAPACITY, LEARNING_RATE,\n",
        "                      GAMMA, EPSILON_START, EPSILON_END, EPSILON_DECAY,\n",
        "                      TARGET_UPDATE_FREQ)\n",
        "          for i in env.agent_ids}\n",
        "\n",
        "# Print agent information\n",
        "for agent_id, agent in agents.items():\n",
        "    print(f\"Agent {agent_id}:\")\n",
        "    print(f\"  Observation dimension: {agent.observation_dim}\")\n",
        "    print(f\"  Action dimension: {agent.action_dim}\")\n",
        "    print(f\"  Device: {agent.device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Loop\n",
        "\n",
        "Let's train our agents using Independent Q-Learning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_rewards(rewards_history, avg_rewards_history):\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(rewards_history, alpha=0.3, label='Episode Rewards')\n",
        "    plt.plot(avg_rewards_history, label='Avg Rewards (50 episodes)')\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Reward')\n",
        "    plt.title('IQL Training Performance')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training parameters\n",
        "NUM_EPISODES = 500\n",
        "LEARN_EVERY_N_STEPS = 4\n",
        "LOG_FREQ = 50\n",
        "RENDER_FREQ = 100\n",
        "\n",
        "episode_rewards_history = []\n",
        "avg_rewards_history = []\n",
        "total_steps = 0\n",
        "\n",
        "print(f\"Starting IQL Training for {NUM_EPISODES} episodes...\")\n",
        "\n",
        "for episode in range(NUM_EPISODES):\n",
        "    observations = env.reset()\n",
        "    episode_rewards = {i: 0 for i in env.agent_ids}\n",
        "    done = False\n",
        "    \n",
        "    while not done:\n",
        "        total_steps += 1\n",
        "        # 1. Select action for each agent\n",
        "        joint_action = {agent_id: agent.select_action(observations[agent_id])\n",
        "                      for agent_id, agent in agents.items()}\n",
        "        \n",
        "        # 2. Step the environment\n",
        "        next_observations, rewards, dones, info = env.step(joint_action)\n",
        "        \n",
        "        # 3. Store experience in each agent's buffer\n",
        "        for agent_id, agent in agents.items():\n",
        "            agent.replay_buffer.add(observations[agent_id], joint_action[agent_id],\n",
        "                                  rewards[agent_id], next_observations[agent_id],\n",
        "                                  dones[agent_id])\n",
        "        \n",
        "        # Update observations\n",
        "        observations = next_observations\n",
        "        \n",
        "        # 4. Perform learning step for each agent\n",
        "        if total_steps % LEARN_EVERY_N_STEPS == 0:\n",
        "            for agent_id, agent in agents.items():\n",
        "                if len(agent.replay_buffer) >= BATCH_SIZE:\n",
        "                    agent.learn(BATCH_SIZE)\n",
        "        \n",
        "        # Update episode rewards\n",
        "        for agent_id in env.agent_ids:\n",
        "            episode_rewards[agent_id] += rewards[agent_id]\n",
        "        \n",
        "        # Check if episode is finished\n",
        "        done = dones['__all__']\n",
        "        \n",
        "        # Render environment periodically\n",
        "        if episode % RENDER_FREQ == 0 and episode > 0 and not done:\n",
        "            clear_output(wait=True)\n",
        "            print(f\"Episode {episode}/{NUM_EPISODES}\")\n",
        "            env.render()\n",
        "            time.sleep(0.1)  # Add a small delay for better visualization\n",
        "    \n",
        "    # End of episode\n",
        "    # Decay epsilon for all agents\n",
        "    for agent in agents.values():\n",
        "        agent.decay_epsilon()\n",
        "    \n",
        "    # Log results\n",
        "    total_episode_reward = sum(episode_rewards.values())\n",
        "    episode_rewards_history.append(total_episode_reward)\n",
        "    \n",
        "    # Calculate average reward over last 50 episodes\n",
        "    if len(episode_rewards_history) >= 50:\n",
        "        avg_reward = np.mean(episode_rewards_history[-50:])\n",
        "    else:\n",
        "        avg_reward = np.mean(episode_rewards_history)\n",
        "    \n",
        "    avg_rewards_history.append(avg_reward)\n",
        "    \n",
        "    if (episode + 1) % LOG_FREQ == 0:\n",
        "        clear_output(wait=True)\n",
        "        print(f\"Episode {episode + 1}/{NUM_EPISODES} | \"\n",
        "              f\"Avg Reward (Last 50): {avg_reward:.2f} | \"\n",
        "              f\"Epsilon: {agents[0].epsilon:.3f}\")\n",
        "        \n",
        "        # Plot rewards\n",
        "        plot_rewards(episode_rewards_history, avg_rewards_history)\n",
        "\n",
        "print(\"Training finished.\")\n",
        "\n",
        "# Final reward plot\n",
        "plot_rewards(episode_rewards_history, avg_rewards_history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluation\n",
        "\n",
        "Now let's evaluate the trained agents by visualizing their behavior."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_agents(env, agents, num_episodes=5, render=True):\n",
        "    \"\"\"Evaluate the agents' performance.\"\"\"\n",
        "    total_rewards = []\n",
        "    \n",
        "    for episode in range(num_episodes):\n",
        "        observations = env.reset()\n",
        "        episode_rewards = {i: 0 for i in env.agent_ids}\n",
        "        done = False\n",
        "        step = 0\n",
        "        \n",
        "        if render:\n",
        "            print(f\"\\nEvaluation Episode {episode + 1}/{num_episodes}, Step {step}\")\n",
        "            env.render()\n",
        "            time.sleep(0.5)\n",
        "        \n",
        "        while not done:\n",
        "            step += 1\n",
        "            # Use greedy actions for evaluation (epsilon = 0)\n",
        "            joint_action = {}\n",
        "            for agent_id, agent in agents.items():\n",
        "                # Save original epsilon\n",
        "                original_epsilon = agent.epsilon\n",
        "                agent.epsilon = 0.0  # Set to 0 for greedy action selection\n",
        "                joint_action[agent_id] = agent.select_action(observations[agent_id])\n",
        "                agent.epsilon = original_epsilon  # Restore original epsilon\n",
        "            \n",
        "            next_observations, rewards, dones, info = env.step(joint_action)\n",
        "            \n",
        "            # Update episode rewards\n",
        "            for agent_id in env.agent_ids:\n",
        "                episode_rewards[agent_id] += rewards[agent_id]\n",
        "            \n",
        "            # Update observations\n",
        "            observations = next_observations\n",
        "            \n",
        "            # Render if required\n",
        "            if render:\n",
        "                clear_output(wait=True)\n",
        "                print(f\"Evaluation Episode {episode + 1}/{num_episodes}, Step {step}\")\n",
        "                print(f\"Actions: {joint_action}\")\n",
        "                env.render()\n",
        "                time.sleep(0.5)  # Slow down for visualization\n",
        "            \n",
        "            # Check if episode is done\n",
        "            done = dones['__all__']\n",
        "        \n",
        "        # Episode finished\n",
        "        total_reward = sum(episode_rewards.values())\n",
        "        total_rewards.append(total_reward)\n",
        "        \n",
        "        if render:\n",
        "            print(f\"Episode Rewards: {episode_rewards}\")\n",
        "            print(f\"Total Reward: {total_reward}\")\n",
        "    \n",
        "    avg_reward = np.mean(total_rewards)\n",
        "    print(f\"\\nEvaluation Results:\")\n",
        "    print(f\"Average Total Reward: {avg_reward:.2f}\")\n",
        "    return avg_reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate the trained agents\n",
        "avg_reward = evaluate_agents(env, agents, num_episodes=3, render=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n",
        "\n",
        "In this notebook, we have seen how to implement and train Independent Q-Learning (IQL) agents in a multi-agent setting. The key insights are:\n",
        "\n",
        "1. Each agent has its own Q-network and learns independently without explicitly considering other agents\n",
        "2. Agents interact through the environment, indirectly affecting each other's state and rewards\n",
        "3. Over time, agents learn to navigate the environment and reach their goals while avoiding obstacles\n",
        "\n",
        "IQL provides a simple yet effective baseline for multi-agent reinforcement learning. However, it has limitations in highly coordinated tasks because each agent treats other agents as part of the environment rather than explicitly modeling their behavior."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}